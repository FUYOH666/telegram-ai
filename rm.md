Роадмап улучшений Telegram AI Assistant
Оценка текущей архитектуры
Сильные стороны
Модульная архитектура с четким разделением ответственности:

Каждый модуль отвечает за одну область (Memory, AIClient, SalesFlow, RateLimiter)
Dependency Injection через конструкторы - легко тестировать и заменять компоненты
Централизованная конфигурация через pydantic-settings с валидацией
Orchestration в client.py - координация модулей в одном месте
Архитектурный паттерн: Модульная монолитная архитектура

Подходит для персонального ассистента (не нужны микросервисы)
Легко добавлять новые модули без изменения существующих
Простое развертывание и отладка
Низкая латентность между модулями
Расширяемость:

Новые модули добавляются через dependency injection в TelegramUserClient.__init__()
Конфигурация расширяется через новые классы в config.py
Обработчики событий регистрируются в _register_handlers()
Рекомендации по архитектуре
Текущая архитектура оптимальна для проекта:

Модули лучше микросервисов для персонального ассистента (нет необходимости в масштабировании на тысячи пользователей)
Легко модифицировать через добавление новых модулей
Возможные улучшения: абстракции (интерфейсы) для заменяемых компонентов, но не критично
Не требуется переход на микросервисы:

Микросервисы добавят сложность без выгоды для персонального использования
Модульная архитектура проще в разработке и отладке
Единая БД проще в управлении чем распределенная
---

Этап 1: Высокий приоритет (Важно + Легко интегрируется)
1.1 Автоматическое извлечение слотов через LLM
Проблема: Слоты заполняются только через явные вопросы пользователю, увеличивая количество шагов в диалоге.

Решение: Использовать LLM для автоматического извлечения слотов из сообщений (бюджет, даты, контакты, цели).

Реализация:

Создать src/telegram_ai/slot_extractor.py с классом SlotExtractor
Метод extract_slots(message: str, intent: str, required_slots: List[str]) -> Dict[str, Any]
Использовать LLM с промптом для извлечения сущностей из сообщения
Интегрировать в SalesFlow.update_slot() - проверять сообщение на наличие слотов перед запросом
Fallback на ручные вопросы если LLM не нашел слот
Файлы для изменения:

src/telegram_ai/slot_extractor.py - новый модуль
src/telegram_ai/sales_flow.py - добавить метод auto_extract_slots() и интеграцию
src/telegram_ai/client.py - вызывать автоизвлечение перед запросом слотов (строка ~496)
src/telegram_ai/ai_client.py - добавить метод для slot extraction промпта
src/telegram_ai/config.py - добавить SlotExtractorConfig с настройками
config.yaml - добавить секцию slot_extraction
Оценка: Важность: Высокая, Легкость: Средняя, Время: 3-5 дней

---

1.2 Summarization для длинных контекстов
Проблема: При превышении context_window (10 сообщений) старые сообщения теряются. Важная информация из начала диалога пропадает.

Решение: Автоматически создавать краткое резюме старых сообщений и включать его в контекст (как в LangChain ConversationSummaryMemory).

Реализация:

Добавить таблицу ConversationSummary в memory.py для хранения summary
Метод Memory.summarize_old_messages(user_id: int, cutoff_message_id: int) -> str
При превышении context_window вызывать LLM для создания summary старых сообщений
Сохранять summary в БД с привязкой к conversation
Включать summary в начало контекста при get_context() если он существует
Добавить конфигурацию memory.auto_summarize: bool и memory.summary_threshold: int
Файлы для изменения:

src/telegram_ai/memory.py - добавить модель ConversationSummary, методы summarization
src/telegram_ai/memory.py - обновить get_context() для включения summary (строка ~187)
src/telegram_ai/ai_client.py - добавить метод для summarization промпта
src/telegram_ai/config.py - добавить настройки summarization в MemoryConfig
config.yaml - добавить memory.auto_summarize и memory.summary_threshold
Оценка: Важность: Высокая, Легкость: Средняя, Время: 4-6 дней

---

Этап 2: Средний приоритет (Важно + Сложно или Менее важно + Легко)
2.1 ML-based Intent Classification с confidence scores
Проблема: Keyword-based классификация неточна и не дает оценки уверенности.

Решение: Использовать LLM для классификации с confidence scores.

Реализация:

Добавить метод IntentClassifier.classify_with_confidence() возвращающий (Intent, float)
Использовать LLM с промптом для классификации (zero-shot или few-shot)
Fallback на keyword-based при низкой уверенности (<0.7)
Кэшировать результаты для одинаковых сообщений (опционально)
Обновить client.py для использования confidence scores при принятии решений
Файлы для изменения:

src/telegram_ai/intent_classifier.py - добавить LLM-based классификацию с confidence
src/telegram_ai/ai_client.py - добавить метод для intent classification промпта
src/telegram_ai/client.py - использовать confidence scores (строка ~396)
src/telegram_ai/config.py - добавить confidence_threshold в конфигурацию
config.yaml - добавить intent_classifier.confidence_threshold: float
Оценка: Важность: Средняя, Легкость: Сложная, Время: 5-7 дней

---

2.2 Векторный поиск в истории диалогов
Проблема: get_context() возвращает только последние N сообщений по времени, не по релевантности.

Решение: Использовать векторную БД для семантического поиска релевантных сообщений из истории.

Реализация:

Добавить зависимость chromadb в pyproject.toml (легче чем qdrant для начала)
Создать src/telegram_ai/vector_memory.py для векторного хранилища
Векторизовать сообщения при сохранении (использовать embeddings из LLM API или локальную модель)
Добавить метод Memory.get_relevant_context(user_id: int, query: str, limit: int) -> List[Dict]
Комбинировать временной и семантический поиск в get_context() - брать последние N и добавлять релевантные из истории
Добавить конфигурацию для включения/выключения векторного поиска
Файлы для изменения:

pyproject.toml - добавить chromadb>=0.4.0
src/telegram_ai/vector_memory.py - новый модуль для векторного поиска
src/telegram_ai/memory.py - интегрировать векторный поиск в get_context()
src/telegram_ai/ai_client.py - добавить метод для получения embeddings (или использовать отдельный сервис)
src/telegram_ai/config.py - добавить vector_search_enabled в MemoryConfig
config.yaml - добавить memory.vector_search_enabled: bool
Оценка: Важность: Средняя, Легкость: Сложная, Время: 7-10 дней

---

2.3 Distributed Rate Limiting с Redis
Проблема: Rate limiting через SQLite не масштабируется на несколько инстансов приложения.

Решение: Использовать Redis для распределенного rate limiting (опционально, только при необходимости масштабирования).

Реализация:

Добавить зависимость redis в pyproject.toml
Создать src/telegram_ai/redis_rate_limiter.py как альтернативу SQLite-based
Добавить конфигурацию rate_limiting.backend: "sqlite" | "redis"
Рефакторинг RateLimiter для поддержки обоих бэкендов через абстракцию или стратегию
Добавить настройки Redis в config.yaml
Файлы для изменения:

pyproject.toml - добавить redis>=5.0.0
src/telegram_ai/redis_rate_limiter.py - новый модуль для Redis backend
src/telegram_ai/rate_limiter.py - рефакторинг для поддержки разных бэкендов
src/telegram_ai/config.py - добавить backend и redis_url в RateLimitingConfig
config.yaml - добавить секцию rate_limiting.redis
Оценка: Важность: Низкая (только при масштабировании), Легкость: Средняя, Время: 3-4 дня

---

Этап 3: Низкий приоритет (Улучшения качества жизни)
3.1 RAG система для знаний о компании
Проблема: AI отвечает на вопросы о компании/услугах из системного промпта, но информация может быть неактуальной.

Решение: Использовать RAG для поиска актуальной информации перед генерацией ответа.

Реализация:

Создать векторную БД с документацией компании/услуг (использовать chromadb из этапа 2.2)
Добавить модуль src/telegram_ai/rag.py для поиска релевантной информации
Интегрировать в AIClient.get_response() - искать релевантную информацию перед генерацией
Добавлять найденную информацию в системное сообщение как контекст
Добавить конфигурацию для управления RAG системой
Файлы для изменения:

src/telegram_ai/rag.py - новый модуль для RAG поиска
src/telegram_ai/ai_client.py - интегрировать RAG поиск в get_response() (строка ~168)
src/telegram_ai/config.py - добавить RAGConfig с настройками
config.yaml - добавить секцию rag с путями к документации
Оценка: Важность: Средняя, Легкость: Сложная, Время: 5-7 дней

---

Последовательность реализации
Фаза 1: Быстрые победы (2-3 недели) ✅
1.1 - Автоматическое извлечение слотов через LLM ✅
1.2 - Summarization для длинных контекстов ✅
Фаза 2: Долгосрочные улучшения (3-4 недели)
2.1 - ML-based Intent Classification ✅
2.2 - Векторный поиск в истории ✅
Фаза 4: Опционально (при необходимости)
2.3 - Distributed Rate Limiting (только при масштабировании)
3.1 - RAG система
---

Метрики успеха
Автоизвлечение слотов: Сокращение количества вопросов на 30-50%
Summarization: Сохранение важной информации из диалогов >30 сообщений
ML Classification: Улучшение точности классификации на 15-25%
Векторный поиск: Улучшение релевантности контекста на 20-30%
---

Архитектурные принципы при реализации
Модульность: Каждое улучшение - отдельный модуль или расширение существующего
Обратная совместимость: Все улучшения опциональны через конфигурацию
Fallback: Всегда есть fallback на существующее поведение при ошибках
Тестируемость: Каждый новый модуль покрывается тестами
Конфигурируемость: Все параметры настраиваются через config.yaml
---

Зависимости между этапами
1.1 (Slot Extraction) - независим, можно делать первым ✅
1.2 (Summarization) - независим, можно делать параллельно с 1.1 ✅
2.1 (ML Intent) - независим ✅
2.2 (Vector Search) - можно использовать для 3.1 (RAG) ✅
2.3 (Redis Rate Limit) - независим, опционально
3.1 (RAG) - зависит от 2.2 (нужна векторная БД)