"""RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–º–ø–∞–Ω–∏–∏/—É—Å–ª—É–≥–∞—Ö."""

import logging
from pathlib import Path
from typing import Dict, List, Optional

from .vector_memory import VectorMemory

logger = logging.getLogger(__name__)


class RAGSystem:
    """RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏."""

    def __init__(
        self,
        vector_memory: Optional[VectorMemory] = None,
        enabled: bool = True,
        knowledge_base_path: Optional[str] = None,
        max_results: int = 3,
        min_score: float = 0.7,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã.

        Args:
            vector_memory: –≠–∫–∑–µ–º–ø–ª—è—Ä VectorMemory –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            enabled: –í–∫–ª—é—á–∏—Ç—å RAG —Å–∏—Å—Ç–µ–º—É
            knowledge_base_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
            min_score: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π score –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        """
        self.enabled = enabled
        self.knowledge_base_path = Path(knowledge_base_path) if knowledge_base_path else None
        self.max_results = max_results
        self.min_score = min_score
        self.rag_collection = None
        self.rag_collection_name = None

        # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è RAG (–æ—Ç–¥–µ–ª—å–Ω–æ –æ—Ç –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π)
        if enabled and vector_memory:
            self.vector_memory = vector_memory
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è RAG
            self.rag_collection_name = "rag_knowledge_base"
            self._init_rag_collection()
        else:
            self.vector_memory = None
            logger.info("RAG system disabled")

    def _init_rag_collection(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è RAG –µ—Å–ª–∏ –Ω—É–∂–Ω–æ."""
        if not self.enabled or not self.vector_memory or not self.vector_memory.enabled:
            return

        try:
            # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è RAG
            if self.vector_memory.client:
                self.rag_collection = self.vector_memory.client.get_or_create_collection(
                    name=self.rag_collection_name,
                    metadata={"hnsw:space": "cosine"},
                )
                logger.info(f"RAG collection initialized: {self.rag_collection_name}")
            else:
                self.rag_collection = None
        except Exception as e:
            logger.error(f"Error initializing RAG collection: {e}", exc_info=True)
            self.rag_collection = None
            self.enabled = False

    async def load_knowledge_base(self) -> int:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –∏–∑ knowledge_base_path –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if not self.enabled or not self.knowledge_base_path or not self.rag_collection:
            logger.warning("RAG system not enabled or knowledge base path not set")
            return 0

        if not self.knowledge_base_path.exists():
            logger.warning(f"Knowledge base path does not exist: {self.knowledge_base_path}")
            return 0

        loaded_count = 0

        try:
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã (.txt, .md)
            supported_extensions = {".txt", ".md"}
            files = [
                f
                for f in self.knowledge_base_path.rglob("*")
                if f.is_file() and f.suffix.lower() in supported_extensions
            ]

            for file_path in files:
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read()

                    if not content.strip():
                        continue

                    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
                    chunks = self._split_into_chunks(content, chunk_size=500)

                    for i, chunk in enumerate(chunks):
                        doc_id = f"doc_{file_path.stem}_{i}"
                        metadata = {
                            "file_path": str(file_path.relative_to(self.knowledge_base_path)),
                            "chunk_index": i,
                            "total_chunks": len(chunks),
                        }

                        # –ü–æ–ª—É—á–∞–µ–º embedding –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                        embedding = await self.vector_memory.get_embedding(chunk)

                        if embedding:
                            self.rag_collection.add(
                                ids=[doc_id],
                                embeddings=[embedding],
                                documents=[chunk],
                                metadatas=[metadata],
                            )
                        else:
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ embeddings ChromaDB
                            self.rag_collection.add(
                                ids=[doc_id],
                                documents=[chunk],
                                metadatas=[metadata],
                            )

                        loaded_count += 1

                    logger.info(f"Loaded {len(chunks)} chunks from {file_path.name}")
                except Exception as e:
                    logger.error(f"Error loading file {file_path}: {e}", exc_info=True)
                    continue

            logger.info(f"Knowledge base loaded: {loaded_count} chunks from {len(files)} files")
            return loaded_count

        except Exception as e:
            logger.error(f"Error loading knowledge base: {e}", exc_info=True)
            return loaded_count

    def _split_into_chunks(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """
        –†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º.

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –≤ —Å–∏–º–≤–æ–ª–∞—Ö

        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
        """
        if len(text) <= chunk_size:
            return [text]

        chunks = []
        start = 0

        while start < len(text):
            end = start + chunk_size

            # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞–∑–±–∏—Ç—å –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º –¥–ª—è –ª—É—á—à–µ–π –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç–∏
            if end < len(text):
                # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–æ—á–∫—É/–ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ –ø–µ—Ä–µ–¥ end
                last_period = text.rfind(".", start, end)
                last_newline = text.rfind("\n", start, end)

                if last_period > start or last_newline > start:
                    end = max(last_period, last_newline) + 1

            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)

            start = end - overlap

        return chunks

    async def search_relevant_info(self, query: str) -> List[Dict]:
        """
        –ù–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π:
            [{"content": str, "file_path": str, "score": float, ...}, ...]
        """
        if not self.enabled or not self.rag_collection:
            return []

        if not query or not query.strip():
            return []

        try:
            # –ü–æ–ª—É—á–∞–µ–º embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = await self.vector_memory.get_embedding(query)

            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            if query_embedding:
                results = self.rag_collection.query(
                    query_embeddings=[query_embedding],
                    n_results=self.max_results,
                )
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
                results = self.rag_collection.query(
                    query_texts=[query],
                    n_results=self.max_results,
                )

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            found_info = []
            if results and "ids" in results and len(results["ids"]) > 0:
                for i, doc_id in enumerate(results["ids"][0]):
                    if i < len(results["metadatas"][0]) and i < len(results["documents"][0]):
                        metadata = results["metadatas"][0][i]
                        content = results["documents"][0][i]

                        # –ü–æ–ª—É—á–∞–µ–º score
                        distance = 0.0
                        if "distances" in results and len(results["distances"]) > 0:
                            distance = results["distances"][0][i]

                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤ score
                        score = 1.0 - (distance / 2.0) if distance else 0.0

                        if score >= self.min_score:
                            found_info.append(
                                {
                                    "content": content,
                                    "file_path": metadata.get("file_path", ""),
                                    "score": score,
                                    "chunk_index": metadata.get("chunk_index", 0),
                                }
                            )

            logger.debug(
                f"Found {len(found_info)} relevant info chunks for query: {query[:50]}..."
            )
            return found_info

        except Exception as e:
            logger.error(f"Error searching RAG knowledge base: {e}", exc_info=True)
            return []

    def format_context(self, found_info: List[Dict]) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç.

        Args:
            found_info: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        """
        if not found_info:
            return ""

        context_parts = ["\n\nüìö **–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:**\n"]

        for i, info in enumerate(found_info, 1):
            content = info["content"]
            file_path = info.get("file_path", "unknown")
            score = info.get("score", 0.0)

            context_parts.append(f"\n--- –§—Ä–∞–≥–º–µ–Ω—Ç {i} (–∏–∑ {file_path}, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.2f}) ---\n{content}")

        context_parts.append(
            "\n\n‚ö†Ô∏è **–í–∞–∂–Ω–æ:** –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –∫–æ–º–ø–∞–Ω–∏–∏/—É—Å–ª—É–≥–∞—Ö. "
            "–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç —Ç–≤–æ–∏–º –∑–Ω–∞–Ω–∏—è–º, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏–º–µ–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π."
        )

        return "\n".join(context_parts)

